{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://raw.githubusercontent.com/openvinotoolkit/anomalib/development/docs/source/images/logos/anomalib-wide-blue.png\" alt=\"Paris\" class=\"center\"></center>\n",
    "\n",
    "<center>ðŸ’™ A library for benchmarking, developing and deploying deep learning anomaly detection algorithms</center>\n",
    "\n",
    "---\n",
    "\n",
    "> NOTE: \n",
    "This notebook is originally created by @innat on [Kaggle](https://www.kaggle.com/code/ipythonx/mvtec-ad-anomaly-detection-with-anomalib-library/notebook). \n",
    "\n",
    "[Anomalib](https://github.com/openvinotoolkit/anomalib): Anomalib is a deep learning library that aims to collect state-of-the-art anomaly detection algorithms for benchmarking on both public and private datasets. Anomalib provides several ready-to-use implementations of anomaly detection algorithms described in the recent literature, as well as a set of tools that facilitate the development and implementation of custom models. The library has a strong focus on image-based anomaly detection, where the goal of the algorithm is to identify anomalous images, or anomalous pixel regions within images in a dataset.\n",
    "\n",
    "The library supports [`MVTec AD`](https://www.mvtec.com/company/research/datasets/mvtec-ad) (CC BY-NC-SA 4.0) and [`BeanTech`](https://paperswithcode.com/dataset/btad) (CC-BY-SA) for **benchmarking** and `folder` for custom dataset **training/inference**. In this notebook, we will explore `anomalib` training a PADIM model on the `MVTec AD` bottle dataset and evaluating the model's performance. The sections in this notebook explores the steps in `tools/train.py` more in detail. Those who would like to reproduce the results via CLI could use `python tools/train.py --model padim`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing Anomalib\n",
    "Installation can be done in two ways: (i) install via PyPI, or (ii) installing from source. In this notebook, we'll install it from the PyPI version for the sake of simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install anomalib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')\n",
    "logger = logging.getLogger(\"anomalib\")\n",
    "from typing import Any, Dict\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pytorch_lightning import Trainer\n",
    "from torchvision.transforms import ToPILImage\n",
    "\n",
    "from anomalib.config import get_configurable_parameters\n",
    "from anomalib.data import get_datamodule\n",
    "from anomalib.models import get_model\n",
    "from anomalib.pre_processing.transforms import Denormalize\n",
    "from anomalib.utils.callbacks import LoadModelCallback, get_callbacks\n",
    "from anomalib.utils.loggers import get_experiment_logger\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Currently, there are **7** anomaly detection models available in `anomalib` library. Namely, \n",
    "\n",
    "- [CFlow](https://arxiv.org/pdf/2107.12571v1.pdf)\n",
    "- [DFKDE](https://github.com/openvinotoolkit/anomalib/tree/development/anomalib/models/dfkde)\n",
    "- [DFM](https://arxiv.org/pdf/1909.11786.pdf)\n",
    "- [Ganomaly](https://arxiv.org/abs/1805.06725)\n",
    "- [Padim](https://arxiv.org/pdf/2011.08785.pdf)\n",
    "- [Patchcore](https://arxiv.org/pdf/2106.08265.pdf)\n",
    "- [STFPM](https://arxiv.org/pdf/2103.04257.pdf)\n",
    "\n",
    "In this tutorial, we'll be using Padim. Now, let's get their config paths from the respected folders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "In this demonstration, we will choose [Padim](https://arxiv.org/pdf/2011.08785.pdf) model from the above config; which is index 1 in the above dictionary. Let's take a quick look of its config file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Replace the dataset configs with the following.\n",
      "dataset:\n",
      "  name: cubes\n",
      "  format: folder\n",
      "  path: ./datasets/cubes\n",
      "  normal_dir: normal # name of the folder containing normal images.\n",
      "  abnormal_dir: anormal # name of the folder containing abnormal images.\n",
      "  task: classification # classification or segmentation\n",
      "  mask: null #optional\n",
      "  normal_test_dir: null # optional\n",
      "  extensions: null\n",
      "  split_ratio: 0.2  # normal images ratio to create a test split\n",
      "  seed: 0\n",
      "  image_size: 256\n",
      "  train_batch_size: 32\n",
      "  test_batch_size: 32\n",
      "  num_workers: 8\n",
      "  transform_config:\n",
      "    train: null\n",
      "    val: null\n",
      "  create_validation_set: true\n",
      "  tiling:\n",
      "    apply: false\n",
      "    tile_size: null\n",
      "    stride: null\n",
      "    remove_border_count: 0\n",
      "    use_random_tiling: False\n",
      "    random_tile_count: 16\n",
      "\n",
      "model:\n",
      "  name: padim\n",
      "  backbone: resnet18\n",
      "  layers:\n",
      "    - layer1\n",
      "    - layer2\n",
      "    - layer3\n",
      "  normalization_method: min_max # options: [none, min_max, cdf]\n",
      "\n",
      "metrics:\n",
      "  image:\n",
      "    - F1Score\n",
      "    - AUROC\n",
      "  pixel:\n",
      "    - F1Score\n",
      "    - AUROC\n",
      "  threshold:\n",
      "    image_default: 3\n",
      "    pixel_default: 3\n",
      "    adaptive: true\n",
      "\n",
      "project:\n",
      "  seed: 42\n",
      "  path: ./results\n",
      "  log_images_to: [\"local\"]\n",
      "  logger: false # options: [tensorboard, wandb, csv] or combinations.\n",
      "\n",
      "optimization:\n",
      "  openvino:\n",
      "    apply: true\n",
      "\n",
      "# PL Trainer Args. Don't add extra parameter here.\n",
      "trainer:\n",
      "  accelerator: auto # <\"cpu\", \"gpu\", \"tpu\", \"ipu\", \"hpu\", \"auto\">\n",
      "  accumulate_grad_batches: 1\n",
      "  amp_backend: native\n",
      "  auto_lr_find: false\n",
      "  auto_scale_batch_size: false\n",
      "  auto_select_gpus: false\n",
      "  benchmark: false\n",
      "  check_val_every_n_epoch: 1 # Don't validate before extracting features.\n",
      "  default_root_dir: null\n",
      "  detect_anomaly: false\n",
      "  deterministic: false\n",
      "  devices: 1\n",
      "  enable_checkpointing: true\n",
      "  enable_model_summary: true\n",
      "  enable_progress_bar: true\n",
      "  fast_dev_run: false\n",
      "  gpus: null # Set automatically\n",
      "  gradient_clip_val: 0\n",
      "  ipus: null\n",
      "  limit_predict_batches: 1.0\n",
      "  limit_test_batches: 1.0\n",
      "  limit_train_batches: 1.0\n",
      "  limit_val_batches: 1.0\n",
      "  log_every_n_steps: 50\n",
      "  max_epochs: 1\n",
      "  max_steps: -1\n",
      "  max_time: null\n",
      "  min_epochs: null\n",
      "  min_steps: null\n",
      "  move_metrics_to_cpu: false\n",
      "  multiple_trainloader_mode: max_size_cycle\n",
      "  num_nodes: 1\n",
      "  num_processes: null\n",
      "  num_sanity_val_steps: 0\n",
      "  overfit_batches: 0.0\n",
      "  plugins: null\n",
      "  precision: 32\n",
      "  profiler: null\n",
      "  reload_dataloaders_every_n_epochs: 0\n",
      "  replace_sampler_ddp: true\n",
      "  sync_batchnorm: false\n",
      "  tpu_cores: null\n",
      "  track_grad_norm: -1\n",
      "  val_check_interval: 1.0 # Don't validate before extracting features.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "MODEL = 'padim' # 'padim', 'cflow', 'stfpm', 'ganomaly', 'dfkde', 'patchcore'\n",
    "CONFIG_PATH = f\"../../anomalib/models/{MODEL}/cubes_config.yaml\"\n",
    "print(open(CONFIG_PATH, 'r').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could use [get_configurable_parameter](https://github.com/openvinotoolkit/anomalib/blob/development/anomalib/config/config.py#L114) function to read the configs from the path and return them in a dictionary. We use the default config file that comes with Padim implementation, which uses `./datasets/MVTec` as the path to the dataset. We need to overwrite this after loading the config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset': {'name': 'cubes', 'format': 'folder', 'path': '../../datasets/cubes', 'normal_dir': 'normal', 'abnormal_dir': 'anormal', 'task': 'classification', 'mask': None, 'normal_test_dir': None, 'extensions': None, 'split_ratio': 0.2, 'seed': 0, 'image_size': [256, 256], 'train_batch_size': 32, 'test_batch_size': 32, 'num_workers': 8, 'transform_config': {'train': None, 'val': None}, 'create_validation_set': True, 'tiling': {'apply': False, 'tile_size': None, 'stride': None, 'remove_border_count': 0, 'use_random_tiling': False, 'random_tile_count': 16}}, 'model': {'name': 'padim', 'backbone': 'resnet18', 'layers': ['layer1', 'layer2', 'layer3'], 'normalization_method': 'min_max', 'input_size': [256, 256]}, 'metrics': {'image': ['F1Score', 'AUROC'], 'pixel': ['F1Score', 'AUROC'], 'threshold': {'image_default': 3, 'pixel_default': 3, 'adaptive': True}}, 'project': {'seed': 42, 'path': 'results\\\\padim\\\\cubes', 'log_images_to': ['local'], 'logger': False}, 'optimization': {'openvino': {'apply': True}}, 'trainer': {'accelerator': 'auto', 'accumulate_grad_batches': 1, 'amp_backend': 'native', 'auto_lr_find': False, 'auto_scale_batch_size': False, 'auto_select_gpus': False, 'benchmark': False, 'check_val_every_n_epoch': 1, 'default_root_dir': 'results\\\\padim\\\\cubes', 'detect_anomaly': False, 'deterministic': False, 'devices': 1, 'enable_checkpointing': True, 'enable_model_summary': True, 'enable_progress_bar': True, 'fast_dev_run': False, 'gpus': None, 'gradient_clip_val': 0, 'ipus': None, 'limit_predict_batches': 1.0, 'limit_test_batches': 1.0, 'limit_train_batches': 1.0, 'limit_val_batches': 1.0, 'log_every_n_steps': 50, 'max_epochs': 1, 'max_steps': -1, 'max_time': None, 'min_epochs': None, 'min_steps': None, 'move_metrics_to_cpu': False, 'multiple_trainloader_mode': 'max_size_cycle', 'num_nodes': 1, 'num_processes': None, 'num_sanity_val_steps': 0, 'overfit_batches': 0.0, 'plugins': None, 'precision': 32, 'profiler': None, 'reload_dataloaders_every_n_epochs': 0, 'replace_sampler_ddp': True, 'sync_batchnorm': False, 'tpu_cores': None, 'track_grad_norm': -1, 'val_check_interval': 1.0}}\n"
     ]
    }
   ],
   "source": [
    "# pass the config file to model, logger, callbacks and datamodule\n",
    "config = get_configurable_parameters(config_path=CONFIG_PATH)\n",
    "config[\"dataset\"][\"path\"] = \"../../datasets/cubes\"     # or wherever the Custom Dataset is stored.\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset: MVTec AD\n",
    "\n",
    "**MVTec AD** is a dataset for benchmarking anomaly detection methods with a focus on industrial inspection. It contains over **5000** high-resolution images divided into **15** different object and texture categories. Each category comprises a set of defect-free training images and a test set of images with various kinds of defects as well as images without defects. If the dataset is not located in the root datasets directory, anomalib will automatically install the dataset.\n",
    "\n",
    "We could now import the MVtec AD dataset using its specific datamodule implemented in anomalib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['image', 'image_path', 'label'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datamodule = get_datamodule(config)\n",
    "datamodule.setup()          # Downloads the dataset if it's not in the specified `root` directory\n",
    "datamodule.prepare_data()   # Create train/val/test/prediction sets.\n",
    "\n",
    "i, data = next(enumerate(datamodule.val_dataloader()))\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the shapes of the input images and masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22, 3, 256, 256])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"image\"].shape  #   data[\"mask\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could now visualize a normal and abnormal sample from the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Model, Callbacks and Loggers\n",
    "Now, the config file is updated as we want. We can now start model training with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(config)\n",
    "experiment_logger = get_experiment_logger(config)\n",
    "callbacks = get_callbacks(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start training\n",
    "trainer = Trainer(**config.trainer, logger=experiment_logger, callbacks=callbacks)\n",
    "trainer.fit(model=model, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load best model from checkpoint before evaluating\n",
    "load_model_callback = LoadModelCallback(\n",
    "    weights_path=trainer.checkpoint_callback.best_model_path\n",
    ")\n",
    "trainer.callbacks.insert(0, load_model_callback)\n",
    "trainer.test(model=model, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the output images are saved into `results` directory. We could get the output filenames from the directory, read the saved the images and visualize here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config[\"project\"][\"path\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_filenames = [filename for filename in Path(config[\"project\"][\"path\"]).glob(\"**/*.jpg\")]\n",
    "image_filenames[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in image_filenames:\n",
    "    image = Image.open(filename)\n",
    "    display(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f26beec5b578f06009232863ae217b956681fd13da2e828fa5a0ecf8cf2ccd29"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
