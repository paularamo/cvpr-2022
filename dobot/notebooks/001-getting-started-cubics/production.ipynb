{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "384aec93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from typing import Tuple, List\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from IPython import display\n",
    "\n",
    "import warnings\n",
    "from argparse import ArgumentParser, Namespace\n",
    "from importlib import import_module\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "from anomalib.config import get_configurable_parameters\n",
    "from anomalib.deploy.inferencers.base import Inferencer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7dda0a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args() -> Namespace:\n",
    "    \"\"\"Get command line arguments.\n",
    "\n",
    "    Returns:\n",
    "        Namespace: List of arguments.\n",
    "    \"\"\"\n",
    "    parser = ArgumentParser()\n",
    "    # --model_config_path will be deprecated in 0.2.8 and removed in 0.2.9\n",
    "    parser.add_argument(\"--model_config_path\", type=str, required=False, help=\"Path to a model config file\")\n",
    "    parser.add_argument(\"--config\", type=Path, required=True, help=\"Path to a model config file\")\n",
    "    parser.add_argument(\"--weight_path\", type=Path, required=True, help=\"Path to a model weights\")\n",
    "    parser.add_argument(\"--image_path\", type=Path, required=True, help=\"Path to an image to infer.\")\n",
    "    parser.add_argument(\"--save_path\", type=Path, required=False, help=\"Path to save the output image.\")\n",
    "    parser.add_argument(\"--meta_data\", type=Path, required=False, help=\"Path to JSON file containing the metadata.\")\n",
    "    parser.add_argument(\n",
    "        \"--overlay_mask\",\n",
    "        type=bool,\n",
    "        required=False,\n",
    "        default=False,\n",
    "        help=\"Overlay the segmentation mask on the image. It assumes that the task is segmentation.\",\n",
    "    )\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    if args.model_config_path is not None:\n",
    "        warnings.warn(\n",
    "            message=\"--model_config_path will be deprecated in v0.2.8 and removed in v0.2.9. Use --config instead.\",\n",
    "            category=DeprecationWarning,\n",
    "            stacklevel=2,\n",
    "        )\n",
    "        args.config = args.model_config_path\n",
    "\n",
    "    return args\n",
    "\n",
    "\n",
    "def add_label(prediction: np.ndarray, scores: float, font: int = cv2.FONT_HERSHEY_PLAIN) -> np.ndarray:\n",
    "    \"\"\"If the model outputs score, it adds the score to the output image.\n",
    "\n",
    "    Args:\n",
    "        prediction (np.ndarray): Resized anomaly map.\n",
    "        scores (float): Confidence score.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Image with score text.\n",
    "    \"\"\"\n",
    "    text = f\"Confidence Score {scores:.0%}\"\n",
    "    font_size = prediction.shape[1] // 1024 + 1  # Text scale is calculated based on the reference size of 1024\n",
    "    (width, height), baseline = cv2.getTextSize(text, font, font_size, thickness=font_size // 2)\n",
    "    label_patch = np.zeros((height + baseline, width + baseline, 3), dtype=np.uint8)\n",
    "    label_patch[:, :] = (225, 252, 134)\n",
    "    cv2.putText(label_patch, text, (0, baseline // 2 + height), font, font_size, 0, lineType=cv2.LINE_AA)\n",
    "    prediction[: baseline + height, : baseline + width] = label_patch\n",
    "    return prediction\n",
    "\n",
    "\n",
    "def infer(image_path: Path, inferencer: Inferencer, save_path: Optional[Path] = None, overlay: bool = False) -> score:\n",
    "    \"\"\"Perform inference on a single image.\n",
    "\n",
    "    Args:\n",
    "        image_path (Path): Path to image/directory containing images.\n",
    "        inferencer (Inferencer): Inferencer to use.\n",
    "        save_path (Path, optional): Path to save the output image. If this is None, the output is visualized.\n",
    "        overlay (bool, optional): Overlay the segmentation mask on the image. It assumes that the task is segmentation.\n",
    "    \"\"\"\n",
    "    # Perform inference for the given image or image path. if image\n",
    "    # path is provided, `predict` method will read the image from\n",
    "    # file for convenience. We set the superimpose flag to True\n",
    "    # to overlay the predicted anomaly map on top of the input image.\n",
    "    output = inferencer.predict(image=image_path, superimpose=True, overlay_mask=overlay)\n",
    "\n",
    "    # Incase both anomaly map and scores are returned add scores to the image.\n",
    "    if isinstance(output, tuple):\n",
    "        anomaly_map, score = output\n",
    "        output = add_label(anomaly_map, score)\n",
    "        \n",
    "        \n",
    "\n",
    "    # Show or save the output image, depending on what's provided as\n",
    "    # the command line argument.\n",
    "    output = cv2.cvtColor(output, cv2.COLOR_RGB2BGR)\n",
    "    if save_path is None:\n",
    "        cv2.imshow(\"Anomaly Map\", output)\n",
    "        cv2.waitKey(0)  # wait for any key press\n",
    "    else:\n",
    "        # Create directory for parents if it doesn't exist.\n",
    "        save_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        if save_path.suffix == \"\":  # This is a directory\n",
    "            save_path.mkdir(exist_ok=True)  # Create current directory\n",
    "            save_path = save_path / image_path.name\n",
    "        cv2.imwrite(filename=str(save_path), img=output)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cba8f574",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream(model_config_path: str, weight_path: Path, image_path: Path, save_path: Path, meta_data: Path, overlay_mask: bool) -> float:\n",
    "    \n",
    "    #parser.add_argument(\"--model_config_path\", type=str, required=False, help=\"Path to a model config file\")\n",
    "    #parser.add_argument(\"--config\", type=Path, required=True, help=\"Path to a model config file\")\n",
    "    #parser.add_argument(\"--weight_path\", type=Path, required=True, help=\"Path to a model weights\")\n",
    "    #parser.add_argument(\"--image_path\", type=Path, required=True, help=\"Path to an image to infer.\")\n",
    "    #parser.add_argument(\"--save_path\", type=Path, required=False, help=\"Path to save the output image.\")\n",
    "    #parser.add_argument(\"--meta_data\", type=Path, required=False, help=\"Path to JSON file containing the metadata.\")\n",
    "    #parser.add_argument(\n",
    "    #    \"--overlay_mask\",\n",
    "    #    type=bool,\n",
    "    #    required=False,\n",
    "    #    default=False,\n",
    "    #    help=\"Overlay the segmentation mask on the image. It assumes that the task is segmentation.\",\n",
    "    #)\n",
    "    \n",
    "    \"\"\"Stream predictions.\n",
    "\n",
    "    Show/save the output if path is to an image. If the path is a directory, go over each image in the directory.\n",
    "    \"\"\"\n",
    "    # Get the command line arguments, and config from the config.yaml file.\n",
    "    # This config file is also used for training and contains all the relevant\n",
    "    # information regarding the data, model, train and inference details.\n",
    "    args = get_args()\n",
    "    config = get_configurable_parameters(model_config_path) # path config model ******************\n",
    "\n",
    "    # Get the inferencer. We use .ckpt extension for Torch models and (onnx, bin)\n",
    "    # for the openvino models.\n",
    "    extension = weight_path.suffix # weight path *************************\n",
    "    inferencer: Inferencer\n",
    "    if extension in (\".ckpt\"):\n",
    "        module = import_module(\"anomalib.deploy.inferencers.torch\")\n",
    "        TorchInferencer = getattr(module, \"TorchInferencer\")  # pylint: disable=invalid-name\n",
    "        inferencer = TorchInferencer(config=config, model_source=weight_path, meta_data_path=meta_data)\n",
    "\n",
    "    elif extension in (\".onnx\", \".bin\", \".xml\"):\n",
    "        module = import_module(\"anomalib.deploy.inferencers.openvino\")\n",
    "        OpenVINOInferencer = getattr(module, \"OpenVINOInferencer\")  # pylint: disable=invalid-name\n",
    "        inferencer = OpenVINOInferencer(config=config, path=weight_path, meta_data_path=meta_data)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Model extension is not supported. Torch Inferencer exptects a .ckpt file,\"\n",
    "            f\"OpenVINO Inferencer expects either .onnx, .bin or .xml file. Got {extension}\"\n",
    "        )\n",
    "    if image_path.is_dir():\n",
    "        # Write the output to save_path in the same structure as the input directory.\n",
    "        for image in image_path.glob(\"**/*\"):\n",
    "            if image.is_file() and image.suffix in (\".jpg\", \".png\", \".jpeg\"):\n",
    "                # Here save_path is assumed to be a directory. Image subdirectories are appended to the save_path.\n",
    "                save_path = Path(save_path / image.relative_to(image_path).parent) if save_path else None\n",
    "                score = infer(image, inferencer, save_path, overlay_mask)\n",
    "    elif image_path.suffix in (\".jpg\", \".png\", \".jpeg\"):\n",
    "        score = infer(image_path, inferencer, save_path, overlay_mask)\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Image extension is not supported. Supported extensions are .jpg, .png, .jpeg.\"\n",
    "            f\" Got {image_path.suffix}\"\n",
    "        )\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9ebf75f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--model_config_path MODEL_CONFIG_PATH] --config CONFIG --weight_path WEIGHT_PATH\n",
      "                             --image_path IMAGE_PATH [--save_path SAVE_PATH] [--meta_data META_DATA]\n",
      "                             [--overlay_mask OVERLAY_MASK]\n",
      "ipykernel_launcher.py: error: the following arguments are required: --config, --weight_path, --image_path\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Intel\\anomalib_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3406: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "score = stream(model_config_path=\"C:\\Intel\\anomalib\\anomalib\\models\\padim\\cubes_config.yaml\",\n",
    "               weight_path=\"C:\\Intel\\anomalib\\notebooks\\001-getting-started\\results\\padim\\cubes\\weights\\model-v1.ckpt\",\n",
    "               image_path=\"C:\\Intel\\anomalib\\datasets\\cubes\\anormal\\input_20220613161135.jpg\",\n",
    "               save_path = None,\n",
    "               meta_data = None,\n",
    "               overlay_mask = None)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2dad53a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Intel\\anomalib_env\\lib\\site-packages\\torchmetrics\\utilities\\prints.py:36: UserWarning: Torchmetrics v0.9 introduced a new argument class property called `full_state_update` that has\n",
      "                not been set for this class (AdaptiveThreshold). The property determines if `update` by\n",
      "                default needs access to the full metric state. If this is not the case, significant speedups can be\n",
      "                achieved and we recommend setting this to `False`.\n",
      "                We provide an checking function\n",
      "                `from torchmetrics.utilities import check_forward_no_full_state`\n",
      "                that can be used to check if the `full_state_update=True` (old and potential slower behaviour,\n",
      "                default for now) or if `full_state_update=False` can be used safely.\n",
      "                \n",
      "  warnings.warn(*args, **kwargs)\n",
      "C:\\Intel\\anomalib_env\\lib\\site-packages\\torchmetrics\\utilities\\prints.py:36: UserWarning: Metric `PrecisionRecallCurve` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "C:\\Intel\\anomalib_env\\lib\\site-packages\\torchmetrics\\utilities\\prints.py:36: UserWarning: Torchmetrics v0.9 introduced a new argument class property called `full_state_update` that has\n",
      "                not been set for this class (AnomalyScoreDistribution). The property determines if `update` by\n",
      "                default needs access to the full metric state. If this is not the case, significant speedups can be\n",
      "                achieved and we recommend setting this to `False`.\n",
      "                We provide an checking function\n",
      "                `from torchmetrics.utilities import check_forward_no_full_state`\n",
      "                that can be used to check if the `full_state_update=True` (old and potential slower behaviour,\n",
      "                default for now) or if `full_state_update=False` can be used safely.\n",
      "                \n",
      "  warnings.warn(*args, **kwargs)\n",
      "C:\\Intel\\anomalib_env\\lib\\site-packages\\torchmetrics\\utilities\\prints.py:36: UserWarning: Torchmetrics v0.9 introduced a new argument class property called `full_state_update` that has\n",
      "                not been set for this class (MinMax). The property determines if `update` by\n",
      "                default needs access to the full metric state. If this is not the case, significant speedups can be\n",
      "                achieved and we recommend setting this to `False`.\n",
      "                We provide an checking function\n",
      "                `from torchmetrics.utilities import check_forward_no_full_state`\n",
      "                that can be used to check if the `full_state_update=True` (old and potential slower behaviour,\n",
      "                default for now) or if `full_state_update=False` can be used safely.\n",
      "                \n",
      "  warnings.warn(*args, **kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Intel\\anomalib\\tools\\inference.py\", line 173, in <module>\n",
      "    stream()\n",
      "  File \"C:\\Intel\\anomalib\\tools\\inference.py\", line 126, in stream\n",
      "    infer(args.image_path, inferencer, args.save_path, args.overlay_mask)\n",
      "  File \"C:\\Intel\\anomalib\\tools\\inference.py\", line 153, in infer\n",
      "    file = open(\"C:\\Intel\\anomalib\\datasets\\cubes\\testing\\anomalous.txt\", \"w\")\n",
      "OSError: [Errno 22] Invalid argument: 'C:\\\\Intel\\x07nomalib\\\\datasets\\\\cubes\\testing\\x07nomalous.txt'\n"
     ]
    }
   ],
   "source": [
    "!python \"C:\\Intel\\anomalib\\tools\\inference.py\" \\\n",
    "--config \"C:\\Intel\\anomalib\\anomalib\\models\\padim\\cubes_config.yaml\" \\\n",
    "--weight_path \"C:\\Intel\\anomalib\\notebooks\\001-getting-started\\results\\padim\\cubes\\weights\\model-v1.ckpt\" \\\n",
    "--image_path \"C:\\Intel\\anomalib\\datasets\\cubes\\anormal\\input_20220613161135.jpg\"\n",
    "#--image_path \"C:\\Intel\\anomalib\\datasets\\cubes\\anormal\\input_20220613160739.jpg\"\n",
    "#--weight_path \"C:\\Intel\\anomalib\\notebooks\\001-getting-started\\results\\padim\\cubes\\weights\\model.ckpt\" \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a3db1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.creating a video object\n",
    "video = cv2.VideoCapture(1) \n",
    "# 2. Variable\n",
    "a = 0\n",
    "# 3. While loop\n",
    "while True:\n",
    "    a = a + 1\n",
    "    # 4.Create a frame object\n",
    "    check, frame = video.read()\n",
    "    # Converting to grayscale\n",
    "    #gray = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "    # 5.show the frame!\n",
    "    cv2.imshow(\"Capturing\",frame)\n",
    "    # 6.for playing \n",
    "    key = cv2.waitKey(1)\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "# 7. image saving\n",
    "showPic = cv2.imwrite(\"filename.jpg\",frame)\n",
    "print(showPic)\n",
    "# 8. shutdown the camera\n",
    "video.release()\n",
    "cv2.destroyAllWindows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb0fad5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9421603679656982\n"
     ]
    }
   ],
   "source": [
    "f = open(\"anomalous.txt\")\n",
    "text = f.read()\n",
    "print(text)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7aea36",
   "metadata": {},
   "outputs": [],
   "source": [
    "_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1203282d",
   "metadata": {},
   "outputs": [],
   "source": [
    "_*100 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef750a38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
